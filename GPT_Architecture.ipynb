{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1ACuAIYNEFb",
        "outputId": "cd964087-45c3-423b-8b96-264b1a8b639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XqMvyWnN05L"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\":256, # original 1024\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"drop_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "GIw5BxvIPKuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out, context_length, dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(d_out % num_heads == 0 ), \\\n",
        "     \"d_out must be divisible my num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(d_out,d_out) # linear layer to combine head outputs\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5 , dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "\n",
        "    context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "jVVbOIvmCK3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qW7OfWPOstO",
        "outputId": "82489f26-1ab9-43f4-9d68-fb5a32e4abe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "    norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x * (1+torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
        "        (x+ 0.044715 * torch.pow(x,3))\n",
        "    ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear( 4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "vj1s1PakOMeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5) #A\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK0pasq6PLep",
        "outputId": "db7c95de-260e-466d-d0da-d4e78c6f4a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[-2.9802e-08],\n",
            "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length = cfg[\"context_length\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        dropout = cfg[\"drop_rate\"],\n",
        "        qkv_bias = cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "Epq5wQte-tyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entire GPT Architecture"
      ],
      "metadata": {
        "id": "prN8ljzqle6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    print(in_idx)\n",
        "    batch_size,seq_len = in_idx.shape  # seq_len i.e num_tokens\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "pZRORG2cAgPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o33yv6ZNCZY",
        "outputId": "9247e86b-b643-4677-b8eb-83061a8c659e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\",batch)\n",
        "print(\"\\n Output shape:\",out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb2VlStOMMm0",
        "outputId": "fa4c5c57-c92e-4eec-ee22-09f3489964ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            " Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
            "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
            "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
            "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
            "\n",
            "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
            "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
            "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
            "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total Parameters: {total_params:,} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpZ5tEFNRcZs",
        "outputId": "41ea5488-e2ff-4571-a488-b1067ed17428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 162,419,712 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg1IkzI0RptP",
        "outputId": "baa0955c-6371-4d8d-af84-0b342c48780c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRY4HyPCSCnv",
        "outputId": "76697c68-1792-49da-db86-2cfa2c8afdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 619.58 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,idx,max_new_tokens,context_size):\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:,-context_size:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:,-1,:]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    idx_next = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "\n",
        "    idx = torch.cat((idx,idx_next),dim=1)\n",
        "\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "aeeZcn0VOWKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded: \",encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded tensor shape: \",encoded_tensor.shape)"
      ],
      "metadata": {
        "id": "ntrEoikNV53Q",
        "outputId": "17fcfb15-933d-4655-ba95-f07a46708943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded:  [15496, 11, 314, 716]\n",
            "encoded tensor shape:  torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "out = generate_text(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output: \",out)\n",
        "print(\"Output Length: \",len(out[0]))"
      ],
      "metadata": {
        "id": "XV9doMIhY4Eh",
        "outputId": "a5c367ad-fc30-403e-8a1a-e70f7b64cba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15496,    11,   314,   716]])\n",
            "tensor([[15496,    11,   314,   716, 13240]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991,\n",
            "          6842]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991,\n",
            "          6842, 37891]])\n",
            "tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991,\n",
            "          6842, 37891, 19970]])\n",
            "Output:  tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991,\n",
            "          6842, 37891, 19970, 47477]])\n",
            "Output Length:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "8qpR4ZWvaDFY",
        "outputId": "d31ca1b3-3e17-4057-c6cc-cb237ed4ce52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Laur inhab DistrinetalkQueue bear confidentlyggyenium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "M4VxCF6A5P-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval() #disable dropout during inference"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KIkGgF3w5OAg",
        "outputId": "1b1bcf08-f6e0-4a20-c3bd-5085a3531d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text,tokenizer):\n",
        "  encoded = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids,tokenizer):\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    idx = text_to_token_ids(start_context,tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output Text: \\n\",token_ids_to_text(token_ids,tokenizer))"
      ],
      "metadata": {
        "id": "MwTDIwQn5fLO",
        "outputId": "bf08bb46-3399-4627-9cea-097c932ccf4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
            "          5308]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
            "          5308,  3398]])\n",
            "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
            "          5308,  3398, 13174]])\n",
            "Output Text: \n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a demo small dataset"
      ],
      "metadata": {
        "id": "WUX7LB3QdILl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    text_data = response.read().decode(\"utf-8\")\n",
        "\n",
        "  with open(file_path,\"w\",encoding=\"utf-8\") as file:\n",
        "    file.write(text_data)\n",
        "\n",
        "else:\n",
        "    with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
        "      text_data = file.read()\n"
      ],
      "metadata": {
        "id": "NNFFZNGldG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data[:80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sMnsCXTgd0ni",
        "outputId": "5471b32a-850d-44b3-f7af-e5f6c782bb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow en'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters: \",total_characters)\n",
        "print(\"Tokens: \",total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq5ZqNW0d-10",
        "outputId": "891510ca-2ddd-40dd-a5af-a02a5a7987f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters:  20479\n",
            "Tokens:  5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokernizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range (0,len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i : i+max_length]\n",
        "      target_chunk = token_ids[i+1 : i+1+max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx) :\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "r6gBBLh5ePpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last= drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "PH2E4lQmgPfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length= GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length= GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "MDr93gXOg_iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "  print('Not enough tokens for the training loader.')\n",
        "\n",
        "if total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "  print(\"Not enough tokens for the validation_loader\")"
      ],
      "metadata": {
        "id": "_HMC9H3CilL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Loader: \")\n",
        "for x,y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\n Validation Loader: \")\n",
        "for x,y in val_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_FANRJnjCJy",
        "outputId": "02167676-4468-479c-8a26-bd0f42ce104a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader: \n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            " Validation Loader: \n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnp_WpEd1fbq",
        "outputId": "f62d4947-e171-45fa-bf5f-b52c3edae0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader(data_loader,model,device,num_batches=None):\n",
        "  total_loss = 0\n",
        "\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    print(i)\n",
        "    if i< num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss / num_batches\n"
      ],
      "metadata": {
        "id": "LhVsUjge2ntk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader,model,device)\n",
        "  val_loss = calc_loss_loader(val_loader,model,device)\n",
        "\n",
        "print(\"Training Loss: \", train_loss)\n",
        "print(\"validation Loss: \", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i7DuTEvt5XBa",
        "outputId": "26a90ae2-c6c6-4ca7-a679-c8e7c6bab81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
            "           257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
            "           568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
            "           326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
            "           550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,\n",
            "           290,  4920,  2241,   287,   257,  4489,    64,   319,   262, 34686,\n",
            "         41976,    13,   357, 10915,   314,  2138,  1807,   340,   561,   423,\n",
            "           587, 10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,\n",
            "           286,   465, 13476,     1,   438,  5562,   373,   644,   262,  1466,\n",
            "          1444,   340,    13,   314,   460,  3285,  9074,    13, 46606,   536,\n",
            "          5469,   438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,\n",
            "          3255,   465, 48422,   540,   450,    67,  3299,    13,   366,  5189,\n",
            "          1781,   340,   338,  1016,   284,  3758,   262,  1988,   286,   616,\n",
            "          4286,   705,  1014,   510,    26,   475,   314,   836,   470,   892,\n",
            "           286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,\n",
            "           284,   943, 17034,   318,   477,   314,   892,   286,   526,   383,\n",
            "          1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,    11,\n",
            "         33096,   663,  4808,  3808,    62,   355,   996,   484,   547, 12548,\n",
            "           287,   281, 13079,   410, 12523,   286, 22353,    13,   843,   340,\n",
            "           373,   407,   691,   262,  9074,    13,   536, 48819,   508, 25722,\n",
            "           276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,\n",
            "            11,   379,   262,   938,   402,  1617,   261, 12917,   905,    11,\n",
            "          5025,   502,   878,   402,   271, 10899,   338,   366, 31640,    12,\n",
            "            67, 20811,     1,   284,   910,    11,   351, 10953,   287,   607,\n",
            "          2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,   588,\n",
            "           757, 13984,   198,   198,  5779, 28112],\n",
            "        [41186, 39614,  1386,    11,   287,   262, 13203,  5482,  1044,   276,\n",
            "          5739,    13,   383,  5019, 19001,   286,   262,  5739,  1444,   510,\n",
            "           477,   402,   271, 10899,   338,  1613,     0,   198,   198, 27034,\n",
            "            13,   402,   271, 10899,  9859,   736,   262,  4324,    12,    66,\n",
            "          3325,  1299,    11,  3888,  7263,   257,  4808,    73,   446,   259,\n",
            "         13235,    62,  1336,   286, 11398, 35560,  1000,   292,    11,  7121,\n",
            "           281,  3211,    12, 16337,  1497,    11,   290,   531,    25,   366,\n",
            "          1532,   345,  1302,   994,   345,   460,   655,  6687,   284,   766,\n",
            "           340,    13,   314,   550,   340,   625,   262, 24818,   417,    12,\n",
            "         12239,    11,   475,   339,  3636,   470,  1309,   340,  2652,   526,\n",
            "           198,   198,  5297,   438,    40,   714,   655,  6687,   284,   766,\n",
            "           340,   438,  1169,   717, 18560,   286,  3619,   338,   314,   550,\n",
            "          1683,   550,   284, 14022,   616,  2951,   625,     0, 19672,   484,\n",
            "           550,   262,  1295,   286, 15393,   438, 16706,   262,  4318,  6103,\n",
            "           287,   257, 14005,  7872,   393,  4808, 13698, 10322,  6532,    62,\n",
            "          8263,    12,  3823,    11,   393,   257, 36364,  1396,   417,  4624,\n",
            "           523,   326,   340,  1718,   262,  1657,   832, 41160,   286,  1468,\n",
            "          9932,   316,   666,   966,    13,   383,   517, 12949,  1295,  2627,\n",
            "           262,  4286,  1365,    26,  1865,    11,   355,   616,  2951,  6348,\n",
            "         23840,   284,   262,  2063,    12,  2971,    11,   477,   262, 16704,\n",
            "         14482,  1625,   503,   438,   439,   262, 10818, 20597, 32192,   355,\n",
            "          2709,   330,   871,    11,   262, 15910,   286, 16153,   312,   328,\n",
            "          3780,   416,   543,    11,   351,   884,  2784,  9830,  5032,    11,\n",
            "           339,  5257,   284, 36583,  3241,   422,   262,  1103,  1597,   286,\n",
            "           262,  4286,   284,   617,  2495, 11331,  2768,   590,   286,  3703,\n",
            "            13,  9074,    13,   402,   271, 10899]])\n",
            "1\n",
            "tensor([[  198,  1544, 13818,  4622,    11,  1231, 35987,    11,   290,  7121,\n",
            "           530,   286,   262,  2769,  3211,    12, 49655,  2651,    13,   366,\n",
            "          1858,    25,   787,  3511,  6792,   438,   392,   994,   389,   262,\n",
            "         33204,   345,   588,   526,   198,   198,  1544,  4624,   606,   379,\n",
            "           616, 22662,   290,  3767,   284, 27776,   510,   290,   866,   262,\n",
            "          2119,    11, 12225,   783,   290,   788, 11061,   262,  4286,    13,\n",
            "           198,   198,     1,  2437,   340,  3022,    30,   314,   460,  1560,\n",
            "           345,   287,  1936,  2431,   438,   392,   340,  1422,   470,  1011,\n",
            "           881,  2392,   284,  1645,    13,   764,   764,   764,   314,   460,\n",
            "          3505,   783,   703,  6655,   290, 10607,   314,   373,   618,   314,\n",
            "          1392,  9074,    13,   520,  5493,   338,  3465,    13,  3226,  1781,\n",
            "            11,  2769,   866,    11,   314,   550,  1464,  4808, 31985,    62,\n",
            "           612,   373,   645,   530,   588,   683,   438,  8807,   314,   550,\n",
            "          3750,   351,   262,  4269,    11, 22211,   262,  6678, 40315, 10455,\n",
            "           546,   683,    11, 10597,   314,  2063,  1392,   284,   892,   339,\n",
            "           373,   257,  5287,    11,   530,   286,   262,  1611,   326,   389,\n",
            "          1364,  2157,    13,  2750,   449,   659,    11,   290,   339,  4808,\n",
            "          9776,    62,  1364,  2157,   438, 13893,   339,   550,  1282,   284,\n",
            "          2652,     0,   383,  1334,   286,   514,   550,   284,  1309,  6731,\n",
            "           307, 17676,  1863,   393,   467,   739,    11,   475,   339,   373,\n",
            "          1029,  2029,   262,  1459,   438,   261, 45697, 19369,    11,   355,\n",
            "           345,   910,    13,   198,   198,     1,  5779,    11,   314,  1816,\n",
            "           572,   284,   262,  2156,   287,   616,   749, 34372, 10038,   438,\n",
            "         34330,  3888,    11,  4453, 20927,   502,    11,   379,   262,  3108,\n",
            "           418,   286,  3595,   520,  5493,   338,  3451,   286,  5287,   852,\n",
            "         37492,   416,   262, 13476,   286,   616],\n",
            "        [  503,  4291,   262,  4252, 18250,  8812,   558,    13,   198,   198,\n",
            "            40, 27846,   706,   683,    11,  7425,   416,   465,   938,  1573,\n",
            "            13, 12622, 41379,   293,   373,    11,   287,  1109,    11,  5033,\n",
            "           262,   582,   286,   262,  2589,   438,   292,  3619,  2241,    11,\n",
            "           530,  1244,  1234,   340,    11,   550,   587,   262,   582,   286,\n",
            "           262,  1711,    13,   383,  7099,  6802,   373,   531,   284,   423,\n",
            "          7042,  2241,   379,   616,  1545,   338,  3625,    11,   290,   314,\n",
            "         14028,   611,   257,   256, 11912,   286, 35394,   739, 10724,   262,\n",
            "          6846,   338, 11428,   450,    67,  3299,    13,   887,   645,   438,\n",
            "          1640,   340,   373,   407, 10597,   706,   326,  1785,   326,   262,\n",
            "          4808, 13698, 10322,  6532,    62,  8263,    12,  9649,   550,  9258,\n",
            "           284,  3359,   511,   366,  8642,   521,   829,   526,   198,   198,\n",
            "            40,  2900,   284,  9074,    13,   402,   271, 10899,    11,   508,\n",
            "           550, 18459,  1068,   284,  1577,   257, 23844,   286,  7543,   284,\n",
            "           607,   599,  6321,   287,   262, 17423,    12,  3823,    13,   198,\n",
            "           198,     1,  5195,  4808, 10134,    62,   339,   442, 17758, 12036,\n",
            "          1701,   314,  1965, 25891,    13,   198,   198,  3347,  4376,   607,\n",
            "         26928,   351,   257,  9254,   286,   922,    12, 17047,  8167,  5975,\n",
            "            13,   198,   198,     1,  5812,    11,   339,  1595,   470,  4808,\n",
            "         14150,    62,   284,   783,    11,   345,   760,    26,   290,   314,\n",
            "           765,   683,   284,  2883,  2241,   553,   673,   531,  2407,  2391,\n",
            "            13,   198,   198,    40,  3114,   546,   262, 40894,  2330,    12,\n",
            "          6839, 11978,  2119,    11,   351,   663,  4808, 44769,  8270,    12,\n",
            "           332,   660,    62,   410,  1386, 20394,   262, 23755,   286,   262,\n",
            "         14005,  1801,  2093, 41160,    11,   290,   663, 45592,    12, 14792,\n",
            "          1613,  1424,   287, 19217, 24887, 13431]])\n",
            "2\n",
            "tensor([[   13,   198,   198,     1, 19242,   339,   442, 17758,   465,  5986,\n",
            "          1165,    30,   314,  4398,   470,  1775,   257,  2060,   530,   287,\n",
            "           262,  2156,   526,   198,   198,    32,  3731, 17979,   286, 32315,\n",
            "         12606,  9074,    13,   402,   271, 10899,   338,  1280,   954, 36368,\n",
            "            13,   366,  1026,   338,   465, 11441, 48740,    11,   345,   760,\n",
            "            13,   679,  1139,   484,   821,   407,  4197,   284,   423,   546,\n",
            "            26,   339,   338,  1908,   606,   477,  1497,  2845,   530,   438,\n",
            "          1820, 18560,   438,   392,   326,   314,   423,   284,  1394, 26148,\n",
            "           526,   198,   198,  6653, 11441, 48740,   438, 14295,   338, 48740,\n",
            "           546,   465,  5986,    30,  2011, 20136,   373,  3957,   588,   262,\n",
            "         26394,    12,   301,   971,    13,   314,   531, 10722,   292,  2280,\n",
            "           284,   616,  2583,   408,    25,   366,    40,  1276,  1107,   766,\n",
            "           534, 18560,    11,   345,   760,   526,   198,   198,  3347, 27846,\n",
            "           503,  2048,  4628, 24882,   379,   262,  8812,   558,   810,   607,\n",
            "          5229,    11, 21081,   782,   278,   287,   257, 14263,   276,  5118,\n",
            "            11,   550,  6578,   257, 24518,   290,  7428,   262,  3394, 20096,\n",
            "         39047,   338,  1182,  1022,   465, 14475,    13,   198,   198,     1,\n",
            "          5779,    11,  1282,   981,   339,   338,   407,  2045,   553,   673,\n",
            "           531,    11,   351,   257,  6487,   326,  3088,   284,  7808,   607,\n",
            "         10927,  1108,    26,   290,   314,  3940,   607,  1022,   262, 30623,\n",
            "          2295, 49406,   286,   262,  6899,    11,   290,   510,   262,  3094,\n",
            "         16046,   351,  1059,   430,    12,    66, 12375,   299, 20896,    82,\n",
            "         24357,  1871, 12734,   379,  1123,  9581,    13,   198,   198,   818,\n",
            "           262,  5391,    76,   395,  5228,   286,   607,   275,  2778, 10840,\n",
            "            11, 10371,   257,  1534,  4241,   286, 19217,   290, 18876,  5563,\n",
            "            11,  9174,   530,   286,   262,  5385],\n",
            "        [10197,   832,   262, 46475,   286, 18113,   544,   338, 10953,   314,\n",
            "          2936,  1498,   284,  1986,   262,  1109,   351,  1602, 11227,   414,\n",
            "            13, 23676,  3619,   402,   271, 10899,     0,   383,  1466,   550,\n",
            "           925,   683,   438,   270,   373, 15830,   326,   484,   815, 25722,\n",
            "           683,    13,  9754,   465,   898,  1714,  7380, 30090,   547,  2982,\n",
            "            11,   290,   287,   465,   898,  3292,  8941,   257,  4636, 28582,\n",
            "            13, 18612, 35394,    30,  8673,    13,  1002,   340,   547,    11,\n",
            "           262, 15393,   286,   262,  5977,   373, 29178,  3474,   416,  1310,\n",
            "         40559, 11959,  1636,    11,   508,    11,   287,   477,   922,  4562,\n",
            "            11,  3181,   503,   287,   262, 37090,   257,   845, 22665,   366,\n",
            "           672,   270,  2838,     1,   319,  3619,   438,   505,   286,   883,\n",
            "           905,    88,  6685, 42070,   351,  4738,  6276,   871,   326,   314,\n",
            "           423,  2982,   357,    40,  1839,   470,   910,   416,  4150,     8,\n",
            "          3688,   284,   402,   271, 10899,   338, 12036,    13,   843,   523,\n",
            "           438, 14363, 10568,   852,  5729, 11331, 18893,   540,   438,  1169,\n",
            "          5114, 11835,  3724,   503,    11,   290,    11,   355,  9074,    13,\n",
            "           536,  5469,   550, 11001,    11,   262,  2756,   286,   366,    38,\n",
            "           271, 10899,    82,     1,  1816,   510,    13,   198,   198,  1026,\n",
            "           373,   407, 10597,  1115,   812,  1568,   326,    11,   287,   262,\n",
            "          1781,   286,   257,  1178,  2745,     6,  4686,  1359,   319,   262,\n",
            "         34686, 41976,    11,   340,  6451,  5091,   284,   502,   284,  4240,\n",
            "          1521,   402,   271, 10899,   550,  1813,   510,   465, 12036,    13,\n",
            "          1550, 14580,    11,   340,  1107,   373,   257, 29850,  1917,    13,\n",
            "          1675, 24456,   465,  3656,   561,   423,   587,  1165,  2562,   438,\n",
            "         14363,  3148,  1650,  1010,   550,   587,  6699,   262,  1540,   558,\n",
            "           286,  2282,   326,  9074,    13,   402]])\n",
            "3\n",
            "tensor([[ 1459,   714,  1239,   423,  4499,   326, 18680,   510,    12,  5532,\n",
            "         14000,    13,   764,   764,   764,   198,   198,     1,    40,  2900,\n",
            "           736,   284,   616,   670,    11,   290,  1816,   319, 39136,   278,\n",
            "           290,   285,  4185,  1359,    26,   788,   314,  3114,   379,   262,\n",
            "         50085,   757,    13,   314,  2497,   326,    11,   618,   520,  5493,\n",
            "          8104,   287,   262,   717, 14000,    11,   339,  2993,   655,   644,\n",
            "           262,   886,   561,   307,    13,   679,   550, 17273,   465,  2426,\n",
            "            11, 19233,   340,    11, 11027,   515,   340,    13,  1649,   550,\n",
            "           314,  1760,   326,   351,   597,   286,   616,  1243,    30,  1119,\n",
            "          8020,   470,   587,  4642,   286,   502,   438,    40,   550,   655,\n",
            "          8197,   606,    13,   764,   764,   764,   198,   198,     1,    39,\n",
            "           648,   340,    11,  8759,  2763,    11,   351,   326,  1986,  4964,\n",
            "           502,   314,  3521,   470,   466,  1194, 14000,    13,   383,  8631,\n",
            "          3872,   373,    11,   314,  1422,   470,   760,   810,   284,  1234,\n",
            "           340,   438,    62,    40,   550,  1239,  1900, 44807,  5514,    11,\n",
            "           351,   616,  1650,  1010,   290,   616,  1171,    11,   257,   905,\n",
            "            88, 22870,   286,  9568,  5017,   510,   262,  1109,   438,    40,\n",
            "           655,  9617,  7521,   656,   511,  6698,    13,   764,   764,   764,\n",
            "          3894,    11,  7521,   373,   262,   530,  7090,   883,  2636,  2951,\n",
            "           714,   766,   832,   438,  3826,  3892,   284,   262,  2006, 20212,\n",
            "         19369, 14638,    13,  2094,   470,   345,   760,   703,    11,   287,\n",
            "          3375,   257,  3215,  3303,    11,   772,  6562,  1473,    11,   530,\n",
            "          1139,  2063,   262,   640,   407,   644,   530,  3382,   284,   475,\n",
            "           644,   530,   460,    30,  3894,   438,  5562,   373,   262,   835,\n",
            "           314, 13055,    26,   290,   355,   339,  3830,   612,   290,  7342,\n",
            "           502,    11,   262,  1517,   484,  1444],\n",
            "        [  271, 10899,   550,   366,  7109, 14655,   683,   866,   526,  1114,\n",
            "          9074,    13,   402,   271, 10899,   438,   292,   884,   438, 18108,\n",
            "           407, 11196, 10597,  3016,   257,   614,   706,  3619,   338, 10568,\n",
            "           550,   587,  2077,    13,   632,  1244,   307,   326,   339,   550,\n",
            "          6405,   607,   438, 20777,   339,  8288,   465, 10152,   438, 13893,\n",
            "           339,  1422,   470,   765,   284,   467,   319, 12036,    26,   475,\n",
            "           340,   561,   423,   587,  1327,   284,  5879,   326,   339,   550,\n",
            "          1813,   510,   465, 12036,   780,   339,   550,  6405,   607,    13,\n",
            "           198,   198,  5189,  1781,    11,   611,   673,   550,   407, 17901,\n",
            "           683,   866,    11,   673,   550,  8603,    11,   355,  4544,  9325,\n",
            "           701, 42397,    11,  4054,   284,   366, 26282,   683,   510,     1,\n",
            "           438,  7091,   550,   407,  2957,   683,   736,   284,   262,  1396,\n",
            "           417,    13,  1675,  1234,   262, 14093,   656,   465,  1021,   757,\n",
            "           438, 10919,   257,   410,  5040,   329,   257,  3656,     0,   887,\n",
            "          9074,    13,   402,   271, 10899,  4120,   284,   423,   595,    67,\n",
            "          1328,   340,   438,   392,   314,  2936,   340,  1244,   307,  3499,\n",
            "           284,  1064,   503,  1521,    13,   198,   198,   464,   748,   586,\n",
            "           652,  1204,   286,   262, 34686, 41976, 37733,  2346,   284,   884,\n",
            "         14177,  8233,  1020,  5768,    26,   290,  1719,    11,   319,   616,\n",
            "           835,   284, 22489, 40089,    11,  4978,   257, 19350,   286,  3619,\n",
            "           338,  3652,   436,    81,  5286,  8812,  2114,  1022,   262,   279,\n",
            "          1127,    11,   314,   550,  3589, 28068,   294,  1555,   262,  1306,\n",
            "          1110,    13,   198,   198,    40,  1043,   262,  3155,   379,  8887,\n",
            "         11061,   511, 18057,    12,    83,  6037,    26,   290,  9074,    13,\n",
            "           402,   271, 10899,   338,  7062,   373,   523,  2429,   498,   326,\n",
            "            11,   287,   262, 29543,  2745,    11]])\n",
            "4\n",
            "tensor([[ 1092,   517,   621,   611,   314,  1549,  1239, 12615,   257, 14093,\n",
            "           526,   198,   198,  1870,   465,  8216,  1297,   502,   287,   257,\n",
            "          7644,   326,   339,  1239,  1807,   286,  1997,  2073,    13,   198,\n",
            "           198,    40,  3888,  1497,    11, 43045, 21100,   416,   616, 10059,\n",
            "          9412,    26,   290,   355,   314,  2900,    11,   616,  4151,  3214,\n",
            "           319,   257,  1402,  4286,  2029,   262, 24818,   417,    12, 12239,\n",
            "           438,  1169,   691,  2134,  7163,   262,  8631, 26210,  3425,  9417,\n",
            "           286,   262,  2119,    13,   198,   198,     1,  5812,    11,   416,\n",
            "           449,   659,  2474,   314,   531,    13,   198,   198,  1026,   373,\n",
            "           257, 17548,   286,   257, 50085,   438,   272,  1468, 10032, 50085,\n",
            "            11,  5055,   287,   262,  6290,   739,   257,  3355,    13,   198,\n",
            "           198,     1,  3886,   449,   659,   438,    64,   520,  5493,  2474,\n",
            "           314, 16896,    13,   198,   198,  1544,   373, 10574,    26,   475,\n",
            "           314,  2936,   683,  1969,  2157,   502,    11, 12704,   257,  1310,\n",
            "          2952,    13,   198,   198,     1,  2061,   257,  4240,     0, 14446,\n",
            "           351,   257,  8667,  3951,   438,  4360,   319, 45697, 19369,    13,\n",
            "           921,  9670, 28022,    11,   810,   750,   345,   651,   340,  1701,\n",
            "           198,   198,  1544,  9373,  6364,    25,   366, 27034,    13,   520,\n",
            "          5493,  2921,   340,   284,   502,   526,   198,   198,     1, 10910,\n",
            "           438,    40,  1422,   470,   760,   345,   772,  2993,   262,   520,\n",
            "          5493,    82,    13,   679,   373,   884,   281,  1167,  2588,   856,\n",
            "           607,  2781,   526,   198,   198,     1,    40,  1422,   470,   438,\n",
            "            83,   359,   706,    13,   764,   764,   764,  1375,  1908,   329,\n",
            "           502,   284,  7521,   683,   618,   339,   373,  2636,   526,   198,\n",
            "           198,     1,  2215,   339,   373,  2636,    30,   921,  1701,   198,\n",
            "           198,    40,  1276,   423,  1309,   257],\n",
            "        [ 1310,  1165,   881, 40642,   972,  6654,   832,   616,  5975,    11,\n",
            "           329,   339,  9373,   351,   257,  1207,  8344,   803,  6487,    25,\n",
            "           366,  5297,   438,  7091,   338,   281, 12659,  2829,  1122,    11,\n",
            "           345,   760,    11,  9074,    13,   520,  5493,    13,  2332,   691,\n",
            "          2126,   373,   284,   423,   683,  1760,   416,   257, 38378, 34537,\n",
            "           438,   993,    11,  3595,   520,  5493,     0,  1375,  1807,   340,\n",
            "           262,  1654,   301,   835,   286, 46431,   465, 27951,   438,  1659,\n",
            "         10833,   340,   319,   257,  1308, 27461,  1171,    13,   843,   379,\n",
            "           262,  2589,   314,   373,  4808,  1169,    62, 38378, 34537,   526,\n",
            "           198,   198,     1, 10910,    11,  3595,   520,  5493,   438,   292,\n",
            "           345,   910,    13,  8920,  4808,  5562,    62,   465,  2106,  1701,\n",
            "           198,   198,     1,  2504,   373,   465,  2106,    13,  1375,  4762,\n",
            "           287,   683,    11, 26996,   798,   287,   683,   438,   273,  1807,\n",
            "           673,   750,    13,   887,   673,  3521,   470,  6842,   407,   284,\n",
            "           423,   477,   262,  8263,    12,  9649,   351,   607,    13,  1375,\n",
            "          3521,   470,  6842,   262,  1109,   326,    11,   319,  1401,    77,\n",
            "          3929,  1528,    11,   530,   714,  1464,   651,  1474,  1576,   284,\n",
            "           766,   465,  5986,    13, 23676,  2415,     0,  1375,   338,   655,\n",
            "           257, 24225, 39136,   278,   329,   584, 21441,    13,   520,  5493,\n",
            "           318,   262,   691,  2187,   314,  1683,  2993,   526,   198,   198,\n",
            "             1,  1639,  1683,  2993,    30,   887,   345,   655,   531,   438,\n",
            "             1,   198,   198,    38,   271, 10899,   550,   257, 11040,  8212,\n",
            "           287,   465,  2951,    13,   198,   198,     1,  5812,    11,   314,\n",
            "          2993,   683,    11,   290,   339,  2993,   502,   438,  8807,   340,\n",
            "          3022,   706,   339,   373,  2636,   526,   198,   198,    40,  5710,\n",
            "           616,  3809, 43045,    13,   366,  2215]])\n",
            "5\n",
            "tensor([[22645,    11,   465, 10904,  4252,  6236,   429, 25839,  9230,   808,\n",
            "           276,   416,   257,  8212,   326, 13663,   262,  9040,   286,   257,\n",
            "          2116,    12, 10414,   738,   285, 23968,  4891,    11,   314,  2936,\n",
            "           284,   644,   257,  4922,   339,   550,   262,   976,  3081,   355,\n",
            "           465,  5986,   438,  1169,  3081,   286,  2045,  1190,  4119,    81,\n",
            "           621,   339,   373,    13,   198,   198,  6653,  3656, 27846,   379,\n",
            "           683,  1207,  8344,   803,   306,    11,   475,   465,  2951, 21650,\n",
            "          1613,   607,   284,   262, 18560,    13,   198,   198,     1,  5246,\n",
            "            13,  8759,  2763,  2227,   284,   766,   340,   553,   673,  2540,\n",
            "            11,   355,   611,  2859,  3500,  5223,    13,   679, 28271,   465,\n",
            "         12450,    11,   991, 16755,    13,   198,   198,     1,  5812,    11,\n",
            "          8759,  2763,  1043,   502,   503,   890,  2084,   553,   339,   531,\n",
            "         15376,    26,   788,    11,  6427,   465,  3211,   832,  6164,    25,\n",
            "           366, 16773,   290,   766,   262,  1334,   286,   262,  2156,   526,\n",
            "           198,   198,  1544,  3751,   340,   284,   502,   351,   257,  1611,\n",
            "           286, 24354, 20154, 11293,    25,   262,  7837,    12,  9649,    11,\n",
            "           262,  5486,    12,    83, 29080,    11,   262,  6576,    12,   565,\n",
            "           418,  1039,    11,   262,  4057,  2655,    12,  8439,   274,   438,\n",
            "           439,   262,  3716,  7106,  6637,   286,   262, 45172,   338,  5928,\n",
            "          3773,    13,   843,  8797,   616,  4240,  3432,   262,  2938, 17547,\n",
            "           339,   531,    11,  9644,   503,   465,  7721,   257,  1310,    25,\n",
            "           366,  5297,    11,   314,  1107,   836,   470,   766,   703,   661,\n",
            "          6687,   284,  2107,  1231,   326,   526,   198,   198,  5779,   438,\n",
            "           270,   373,   655,   262,   886,   530,  1244,   423,  1674, 15898,\n",
            "           329,   683,    13,  5514,   339,   373,    11,   832,   340,   477,\n",
            "           290,   287, 15275,   286,   340,   477],\n",
            "        [  286,  1762,    30,  2011, 29483,  2540,   284,   467,   257,  1310,\n",
            "          4295,   438,    40,  2936, 10927,   290,  8627,    13,   198,   198,\n",
            "             1,  7454,    11,   618,   314,  3114,   510,    11,   314,  3947,\n",
            "           284,   766,   257,  8212,  2157,   465,  1969, 12768,   680, 21213,\n",
            "           438,   292,   611,   339,   550,   262,  3200,    11,   290,   547,\n",
            "         28297,  2241,   416,  4769,   340,   736,   422,   502,    13,  1320,\n",
            "         41851,   515,   502,   991,   517,    13,   383,  3200,    30,  4162,\n",
            "            11,   314,   550,   257,  3200,  2861,  8208,   286,   465,     0,\n",
            "           314, 37901,   379,   262, 21978, 44896,    11,   290,  3088,   617,\n",
            "           286,   616, 49025,  5330, 15910,    13,   887,   484,  4054,   502,\n",
            "            11,   484,  1067, 11137,    13,   314,  2497,   326,   339,  2492,\n",
            "           470,  4964,   262,   905,    88, 10340,   438,    40,  3521,   470,\n",
            "         11786,   465,  3241,    26,   339,   655,  4030,   465,  2951,   319,\n",
            "           262,  1327, 22674,  1022,    13,  5845,   547,   262,  3392,   314,\n",
            "           550,  1464,   427,   343,  9091,    11,   393,  5017,   510,   351,\n",
            "           617,  9105,  7521,    13,   843,   703,   339,  2497,   832,   616,\n",
            "          7363,     0,   198,   198,     1,    40,  3114,   510,   757,    11,\n",
            "           290,  4978,  6504,   286,   326, 17548,   286,   262, 50085, 10938,\n",
            "           319,   262,  3355,  1474,   465,  3996,    13,  2399,  3656,  1297,\n",
            "           502, 20875,   340,   373,   262,   938,  1517,   339,   550,  1760,\n",
            "           438,  3137,   257,  3465,  2077,   351,   257, 17275,  1021,    11,\n",
            "           618,   339,   373,   866,   287,  6245,   684, 10695, 20222,   422,\n",
            "           257,  2180,  2612,  1368,    13,  2329,   257,  3465,     0,   887,\n",
            "           340,  4952,   465,  2187,  2106,    13,  1318,   389,   812,   286,\n",
            "          5827, 40987,   913, 30802,   287,   790,  1627,    13,   317,   582,\n",
            "           508,   550,  1509,   388,   351,   262]])\n",
            "6\n",
            "tensor([[  673,  1908,   329,   345,  1701,   198,   198,     1,  5297,   438,\n",
            "         37121,  1035, 27339,   284,   262, 21296,    13,  1375,  2227,   683,\n",
            "         29178,  3474,   438,   392,   416,   502,  2474,   198,   198,  1544,\n",
            "         13818,   757,    11,   290,  9617,   736,   465,  1182,   284,   804,\n",
            "           510,   379,   262, 17548,   286,   262, 50085,    13,   366,  1858,\n",
            "           547,  1528,   618,   314,  3521,   470,   804,   379,   326,  1517,\n",
            "           438, 24089,    77,   470,  1986,   340,    13,   887,   314,  4137,\n",
            "          3589,   284,  1234,   340,   994,    26,   290,   783,   340,   338,\n",
            "         30703,   502,   438,    66,  1522,   502,    13,  1320,   338,   262,\n",
            "          1738,  1521,   314,   836,   470, 45553,   903,   597,   517,    11,\n",
            "           616, 13674,  8759,  2763,    26,   393,  2138,   520,  5493,  2241,\n",
            "           318,   262,  1738,   526,   198,   198,  1890,   262,   717,   640,\n",
            "           616, 21696, 20136,   546,   616, 15185,  2900,   656,   257,  2726,\n",
            "          6227,   284,  1833,   683,  1365,    13,   198,   198,     1,    40,\n",
            "          4601,   345,  1549,  1560,   502,   703,   340,  3022,   553,   314,\n",
            "           531,    13,   198,   198,  1544,  6204,  2045,   510,   379,   262,\n",
            "         17548,    11,   290,   665, 24297,  1022,   465,  9353,   257, 17779,\n",
            "           339,   550, 11564,   284,  1657,    13, 24975,   339,  2900,  3812,\n",
            "           502,    13,   198,   198,     1,    40,  1549,  2138,   588,   284,\n",
            "          1560,   345,   438, 13893,   314,  1053,  1464,  9885,   345,   286,\n",
            "          2376, 26927,   616,   670,   526,   198,   198,    40,   925,   257,\n",
            "          1207,  8344,   803, 18342,    11,   543,   339,  2469,   265,  1572,\n",
            "           351,   257,   922,    12, 17047,  8167, 32545,    13,   198,   198,\n",
            "             1,  5812,    11,   314,  1422,   470,  1337,   257, 14787,   618,\n",
            "           314,  4762,   287,  3589,   438,   392,   783,   340,   338,   281,\n",
            "          2087,  9839,  1022,   514,  2474,   198],\n",
            "        [  438,   292,   339,   550,   587,   832,    11,   290,   287, 15275,\n",
            "           286,    11,   465,  5986,   438,   568, 22665,    11,   523, 23332,\n",
            "            11,   523,   595, 18052,    11,   326,   530,   890,   276,   284,\n",
            "          3960,   503,    25,   366,  3856, 44455,   351,   534, 24638,  2474,\n",
            "           355,  1752,   530,   550,   890,   276,   284,   910,    25,   366,\n",
            "          3856, 44455,   351,   534,   670,  2474,   198,   198,  1537,    11,\n",
            "           351,   262,  3960,   319,   616, 11914,    11,   616, 13669,  6989,\n",
            "           281, 10059,  2198,    13,   198,   198,     1,  1212,   318,   616,\n",
            "           898, 49451,   553,   339,   531,    11,  3756,   502,   656,   257,\n",
            "          3223,  8631,  2119,   379,   262,   886,   286,   262,   781,   273,\n",
            "           312,   410, 12523,    13,   632,   373,  6616,   290,  7586,   290,\n",
            "         11620,    88,    25,   645,   366, 34435,  8172,   645,   865,   291,\n",
            "            12,    64,    12,  1671,   330,    11,  4844,   286,   262,  1633,\n",
            "           286, 24380,   329, 20728,   287,   257,  4286, 10273,   438, 29370,\n",
            "           477,    11,   645,  1551,  1051,   286,  1683,  1719,   587,   973,\n",
            "           355,   257,  8034,    13,   198,   198,   464,  1109,  3181,  1363,\n",
            "           284,   502,   262,  4112,   957,  1483,   286,  3619,   338,  2270,\n",
            "           351,   465,  1468,  1204,    13,   198,   198,     1,  3987,   470,\n",
            "           345,  1683, 45553,   903,   351,  7521,   597,   517,  1701,   314,\n",
            "          1965,    11,   991,  2045,   546,   329,   257, 12854,   286,   884,\n",
            "          3842,    13,   198,   198,     1, 12295,   553,   339,   531, 11589,\n",
            "            13,   198,   198,     1,  5574,  1660,    12, 49903,   438,   273,\n",
            "          2123, 10813,  1701,   198,   198,  6653,  6563,  2951,  6348,  5391,\n",
            "            11,   290,   465, 25839,   279,  3021,   257,  1310,   739,   511,\n",
            "         22665,  4252, 10899,    13,   198,   198,     1, 12295,   892,   286,\n",
            "           340,    11,   616, 13674,  5891,   438]])\n",
            "7\n",
            "tensor([[  314,  4752,   340,  6777,    13,   632,   373,   407,   326,   616,\n",
            "          2583,   408,   373,   366, 47914,  1298,   319,   326,   966,   314,\n",
            "           714,   423,  1813,  4544,  9325,   701,   262, 40830, 12719,  3874,\n",
            "            13,   632,   373,   655,   780,   673,   373,  4808,  1662,    62,\n",
            "          3499,   438,   361,   314,   743,   307, 41746, 12004,   262,  6473,\n",
            "           438,  5562,   314,  1043,   607,   523,    13,  1114,  3619,    11,\n",
            "           477,   465,  1204,    11,   550,   587, 11191,   416,  3499,  1466,\n",
            "            25,   484,   550, 26546,  1068,   465,  1242,    11,   340,   550,\n",
            "           587,   302,  1144,   287,   262,  3024,    12,  4803,   286,   511,\n",
            "           512,  1741,    13,   843,   340,   373,  4361,  5048,   425,   284,\n",
            "          3465,   644,  1245,   262,   366, 25124,  3101,  8137,   286, 16957,\n",
            "          1696,   414,     1,   357,    40,  9577,  4544,  9325,   701,     8,\n",
            "           373,  1719,   319,   683,    13,   198,   198,    40,   423,  4750,\n",
            "           326,  9074,    13,   402,   271, 10899,   373,  5527,    26,   290,\n",
            "           340,   373,  3393, 34953,   856,   326,   607,  5229,   373, 37895,\n",
            "           422,   428, 25179,   257, 19217,   475,  8904, 14676,    13,   632,\n",
            "           318,    11,   355,   257,  3896,    11,   262,   661,   508, 40987,\n",
            "          1637,   508,   651,   749,   503,   286,   340,    26,   290,  3619,\n",
            "           338, 19992, 31564,   286,   465,  3656,   338,  1263,  5236,  9343,\n",
            "           683,    11,   351,   281,  5585,   286,  2818,   922,    12, 49705,\n",
            "            11,   284, 21595,  1133,   340,   656,  5563,   286,  1242,   290,\n",
            "         13064,    13,  1675,   262,  6846,    11,   314,  1276,   751,    11,\n",
            "           339,  6150,  5365, 31655,    26,   475,   339,   373,  7067, 29396,\n",
            "         18443, 12271,   290, 45592,    12, 14792,  5986,   351,   257,  8839,\n",
            "           326,  7284, 35924,   262, 12306,   395,  4133,    13,   198,   198,\n",
            "             1, 26788,   338,   691, 12226,   318],\n",
            "        [   11, 17728,   257,  8500,  4417,   284,   670,   319,   438, 15464,\n",
            "            11,   355,   340,   547,    11,   523, 16857,   262,  4469,   286,\n",
            "           607,   898,  4286,   438, 18108, 26269,  5223,   287,   281,  8468,\n",
            "          4922,   284,   262,  3359,   286,   428,  3991,  4118,    84, 16579,\n",
            "            13,   383,  4286,   373,   530,   286,  3619,   338,   366, 11576,\n",
            "           395,   553,   355,   465, 21099,  3808,   561,   423,  1234,   340,\n",
            "           438,   270,  7997,    11,   319,   465,   636,    11,   257, 29844,\n",
            "           286, 12749,    11,   257, 22791,   278,   286, 32375,    11,   257,\n",
            "         22486,    11,   965,  2860,  1359,   290,   965,  1397,    11,   326,\n",
            "         14516,   530,   286,   262, 33125,    12,   565,   593,   338, 25304,\n",
            "          4040,   284, 10303,   257, 17972,    13,   632,  1138,    11,   287,\n",
            "          1790,    11,   379,   790,   966,   262,  3512,   286, 14081,  2415,\n",
            "           284,   307, 13055,   366, 11576,   306,     1,   780,   673,   373,\n",
            "         10032,   286,   852, 13055,   366, 34751,   306,     1,   438,   392,\n",
            "          1865,   407,   284,  4425,   281, 22037,   286,   262, 32073,    13,\n",
            "           198,   198,     1,  1026,   338,   262,   938,   339, 13055,    11,\n",
            "           345,   760,   553,  9074,    13,   402,   271, 10899,   531,   351,\n",
            "         27322,   540, 11293,    13,   366,   464,   938,   475,   530,   553,\n",
            "           673, 19267,  5223,   438,     1,  4360,   262,   584,  1595,   470,\n",
            "           954,    11,   780,   339,  6572,   340,   526,   198,   198,     1,\n",
            "         49174,   276,   340,  1701,   314,   373,   546,   284,  1061,   510,\n",
            "           428, 18437,   618,   314,  2982,   257,  2366,  9662,   290,  2497,\n",
            "          3619,  2241,   319,   262, 11387,    13,   198,   198,  1722,   339,\n",
            "          6204,   612,    11,   465,  2832,   287,   262, 16511,   286,   465,\n",
            "         11555,   303,  7821, 13209,    11,   262,  7888,  7586,  9813,   286,\n",
            "          4190,  7121,   736,   422,   465,  2330]])\n",
            "8\n",
            "tensor([[12036,   683,     0,  3226,  1781,   314,  4001,   284,   466,   262,\n",
            "          4286,   329,  2147,   438,    40,  1297,  9074,    13,   520,  5493,\n",
            "           523,   618,   673,  2540,   284,   336,   321,   647,  1223,   546,\n",
            "           607,  8098,    13,   314,  3505,  1972,   572,   257, 40426, 10956,\n",
            "          9546,   546,   262, 15393,   852,  4808,  3810,    62,   438,  1219,\n",
            "            11,   314,   373, 19716,   306,    11,   616, 13674,  8759,  2763,\n",
            "             0,   314,   373, 24380,   284,  3589,   588,   530,   286,   616,\n",
            "           898,  1650,  1010,    13,   198,   198,     1,  6423,   314,   373,\n",
            "          2077,   510,   290,  1364,  3436,   351,   683,    13,   314,   550,\n",
            "          1908,   477,   616, 20348,   287,  5963,    11,   290,   314,   550,\n",
            "           691,   284,   900,   510,   262,  1396,   417,   290,   651,   284,\n",
            "           670,    13,   679,   550,   587,  2636,   691,  8208,    12, 14337,\n",
            "          2250,    11,   290,   339,  3724,  6451,    11,   286,  2612,  4369,\n",
            "            11,   523,   326,   612,   550,   587,   645, 15223,   670,   286,\n",
            "          8166,   438, 14363,  1986,   373,  1598,   290, 36519,    13,   314,\n",
            "           550,  1138,   683,  1752,   393,  5403,    11,   812,   878,    11,\n",
            "           290,  1807,   683, 32081,   290, 44852,    88,    13,  2735,   314,\n",
            "          2497,   326,   339,   373, 21840,    13,   198,   198,     1,    40,\n",
            "           373,  9675,   379,   717,    11,   351,   257,  6974, 19713, 14676,\n",
            "            25,  9675,   284,   423,   616,  1021,   319,   884,   257,   705,\n",
            "         32796,  2637,  3244,   465,  6283,  1204,    12, 46965,  9449,  2540,\n",
            "           284,  2689,   502, 24506,   306,   438,   292,   314, 10226,   262,\n",
            "          1182,   287,   314,  2936,   355,   611,   339,   547,  4964,   502,\n",
            "           466,   340,    13,   383, 18098,   373,  3940,   416,   262,  1807,\n",
            "            25,   611,   339,  4808, 22474,    62,  4964,   502,    11,   644,\n",
            "           561,   339,   910,   284,   616,   835],\n",
            "        [  284,  1234,  8737,   656, 19133,   553,   373,   530,   286,   262,\n",
            "          7877,    72,  3150,   339,  8104,   866,  1973,   262, 37918,   411,\n",
            "           290,  8465,   286,   281, 33954,   271,  3973,  9899, 14678, 40556,\n",
            "            12, 11487,    11,   618,    11,   319,   257,  1568,  1110,    11,\n",
            "           314,   550,   757,  1057,   625,   422, 22489, 40089,    26,   290,\n",
            "          9074,    13,   402,   271, 10899,    11,   307,  3723,   319,   683,\n",
            "            11,  2087,   329,   616, 35957,    25,   366, 14295,   318,   523,\n",
            "         34813,   306,  8564,   284,   790,  1296,   286,  8737,   526,   198,\n",
            "           198, 43920,  3619,     0,   632,   550,  1464,   587,   465, 10030,\n",
            "           284,   423,  1466,   910,   884,  1243,   286,   683,    25,   262,\n",
            "          1109,   815,   307,   900,   866,   287,  1070,   268,  2288,    13,\n",
            "          1867,  7425,   502,   783,   373,   326,    11,   329,   262,   717,\n",
            "           640,    11,   339,   581,  4714,   262,  8216,    13,   314,   550,\n",
            "          1775,   683,    11,   523,  1690,    11,  1615,  3364,   739,  2092,\n",
            "           256,  7657,   438,  9776,   340,   262, 11644, 43778,  3465,   326,\n",
            "         26773,   606,   286,   511,  6799,   454,    30,  1400,   438,  1640,\n",
            "            11, 31414,  1576,    11,   340,  2627,  4156,   326,   339,   373,\n",
            "         16245,   286,  9074,    13,   402,   271, 10899,   438,    69,   623,\n",
            "          1576,   407,   284,   766,   607, 41793,    13,   632,   373,   465,\n",
            "           898, 41793,   339,  3947,   284,   307,  1592,  2259,   739,   438,\n",
            "         14363,   898,  9408,   355,   281,  2134,   329,  5482,  4447,   290,\n",
            "           753,  1072,    13,   198,   198,     1,  3666, 13674,    11,  1201,\n",
            "           314,  1053,   442, 17758, 12036,   661,   836,   470,   910,   326,\n",
            "          3404,   546,   502,   438,  9930,   910,   340,   546, 12622, 41379,\n",
            "           293,   553,   373,   465,   691,  5402,    11,   355,   339,  8278,\n",
            "           422,   262,  3084,   290,   336,  8375]])\n",
            "0\n",
            "tensor([[  518,     6, 14707,   588,   257,  2156,   286,  4116,    13,   679,\n",
            "          1422,   470, 10505,   263,    11,   345,  1833,    11,  3595,   520,\n",
            "          5493,   438,   258,   655,  3830,   612, 12703,  4964,    11,   290,\n",
            "           319,   465, 11914,    11,   832,   262, 12768, 21213,    11,   314,\n",
            "          3947,   284,  3285,   262,  1808,    25,   705,  8491,   345,  1654,\n",
            "           345,   760,   810,   345,   821,  2406,   503,  8348,   198,   198,\n",
            "             1,  1532,   314,   714,   423, 13055,   326,  1986,    11,   351,\n",
            "           326,  1808,   319,   340,    11,   314,   815,   423,  1760,   257,\n",
            "          1049,  1517,    13,   383,  1306,  6000,  1517,   373,   284,   766,\n",
            "           326,   314,  3521,   470,   438,   392,   326, 11542,   373,  1813,\n",
            "           502,    13,   887,    11, 11752,    11,   379,   326,  5664,    11,\n",
            "          8759,  2763,    11,   373,   612,  1997,   319,  4534,   314,  3636,\n",
            "           470,   423,  1813,   284,   423,   520,  5493,  6776,   878,   502,\n",
            "            11,   290,   284,  3285,   683,   910,    25,   705,  1026,   338,\n",
            "           407,  1165,  2739,   438,    40,  1183,   905,   345,   703, 30960,\n",
            "           198,   198,     1,  1026,  4808,  9776,    62,  1165,  2739,   438,\n",
            "           270,   561,   423,   587,    11,   772,   611,   339,  1549,   587,\n",
            "          6776,    13,   314, 11856,   510,   616, 20348,    11,   290,  1816,\n",
            "           866,   290,  1297,  9074,    13,   520,  5493,    13,  3226,  1781,\n",
            "           314,  1422,   470,  1560,   607,  4808,  5562,    62,   438,   270,\n",
            "           561,   423,   587,  8312,   284,   607,    13,   314,  2391,   531,\n",
            "           314,  3521,   470,  7521,   683,    11,   326,   314,   373,  1165,\n",
            "          3888,    13,  1375,  2138,  8288,   262,  2126,   438,  7091,   338,\n",
            "           523, 14348,     0,   632,   373,   326,   326,   925,   607,  1577,\n",
            "           502,   262, 50085,    13,   887,   673,   373, 22121,  9247,   379,\n",
            "           407,  1972,   262, 18560,   438,  7091],\n",
            "        [  750,   523,   765,   683,   705, 28060,     6,   416,   617,   530,\n",
            "           905,    88,     0,  1629,   717,   314,   373,  7787,   673,  3636,\n",
            "           470,  1309,   502,   572,   438,   392,   379,   616,   266,   896,\n",
            "             6,   886,   314,  5220, 41379,   293,    13,  3363,    11,   340,\n",
            "           373,   314,   508,  2067, 41379,   293,    25,   314,  1297,  9074,\n",
            "            13,   520,  5493,   339,   373,   262,   705,  4976,     6,   582,\n",
            "            11,   290,   673,  1297,  8276,  2073,    11,   290,   523,   340,\n",
            "          1392,   284,   307,  2081,    13,   764,   764,   764,   843,   339,\n",
            "         13055,   520,  5493,  1231,  1592,  2259,    26,   290,   673,  9174,\n",
            "           262,  4286,  1871,   607,  5229,   338,  1243,    13,   764,   764,\n",
            "         22135,   198,   198,  1544, 45111,  2241,   866,   287,   262,  3211,\n",
            "            12, 16337,  1474,  6164,    11,  8104,   736,   465,  1182,    11,\n",
            "           290, 47425,   278,   465,  5101, 11061,   340,    11,  3114,   510,\n",
            "           379,   262,  4286,  2029,   262, 18205,  1681,    12, 12239,    13,\n",
            "           198,   198,     1,    40,   588,   284, 14996,   326,   520,  5493,\n",
            "          2241,   561,   423,  1813,   340,   284,   502,    11,   611,   339,\n",
            "          1549,   587,  1498,   284,   910,   644,   339,  1807,   326,  1110,\n",
            "           526,   198,   198,  1870,    11,   287,  3280,   284,   257,  1808,\n",
            "           314,  1234,  2063,    12,  1326,  3147,  1146,   438,     1, 44140,\n",
            "           757,  1701,   339, 30050,   503,    13,   366,  2215,   262,   530,\n",
            "          1517,   326,  6774,   502,  6609,  1474,   683,   318,   326,   314,\n",
            "          2993,  1576,   284,  2666,   572,  1701,   198,   198,  1544,  6204,\n",
            "           510,   290,  8104,   465,  1021,   319,   616,  8163,   351,   257,\n",
            "          6487,    13,   366, 10049,   262, 21296,   286,   340,   318,   326,\n",
            "           314,  4808,   321,    62,   991, 12036,   438, 20777, 41379,   293,\n",
            "           338,  1804,   340,   329,   502,     0]])\n",
            "Training Loss:  10.98758347829183\n",
            "validation Loss:  10.98110580444336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader,model,device)\n",
        "    val_loss = calc_loss_loader(val_loader,model,device)\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "DULC7-WEYORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text(model=model,idx=encoded,max_new_tokens=50,context_size=context_size)\n",
        "  decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\",\" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "GoOe8IdvYOCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(nodel, train_loader, val_loader,optimizer,device,num_epochs, eval_freq, eval_iter, start_context,tokenizer):\n",
        "  train_losses,val_losses,track_tokens_seen = [],[],[]\n",
        "  tokens_seen,global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # evaluation\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss,val_loss = evaluate_model(model,train_loader,val_loader,device,eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1} (Step: {global_step:06d}) =>\" f\"Train Loss: {train_loss:.3f}  Val Loss: {val_loss:.3f}\")\n",
        "\n",
        "      generate_and_print_sample(model,tokenizer,device,start_context)\n",
        "\n",
        "  return train_loss, val_loss, track_tokens_seen"
      ],
      "metadata": {
        "id": "dzDLUgUo7GcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004,weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses,val_losses,tokens_seen = train_model(\n",
        "    model,train_loader,val_loader,optimizer,device, num_epochs=num_epochs,eval_freq=5,\n",
        "    eval_iter=5,start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "YCJr2RzgfInx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature Scaling and Top-k Sampling"
      ],
      "metadata": {
        "id": "7CiBueOY4L7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# updating generate_text function.\n",
        "def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=0.0,eos_id=None):\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:,-context_size:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:,-1,:]\n",
        "\n",
        "    if top_k is not None:\n",
        "      top_logits,_ = torch.topk(logits,top_k)\n",
        "      min_val = top_logits[:,-1]\n",
        "      logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
        "\n",
        "    if temperature > 0.0:\n",
        "      logits = logits/temperature\n",
        "      probs = torch.softmax(logits,dim=-1)\n",
        "      idx_next = torch.multinomial(probs,num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(logits,dim=-1,keepdim=True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx,idx_next),dim=1)\n",
        "\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "v0ivWGih4QK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
        "    max_new_tokens = 15,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k = 25,\n",
        "    temperature = 1.4\n",
        ")\n",
        "\n",
        "print(\"Output Text: \",token_ids_to_text(token_ids,tokenizer))"
      ],
      "metadata": {
        "id": "9M0eKoqg59Jk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}